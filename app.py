# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I1s-091EE8oHvL425MQsFjzfHTtcxERj
"""

# Commented out IPython magic to ensure Python compatibility.
# hr_policy_chatbot_ultrasafe.py

# %pip install streamlit pypdf sentence-transformers faiss-cpu plotly pandas groq tiktoken
import os
import io
import json
import uuid
import hashlib
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Tuple
import time

import streamlit as st
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from pypdf import PdfReader

try:
    import faiss  # type: ignore
except Exception:
    faiss = None

try:
    from groq import Groq
except Exception:
    Groq = None

# ------------------------------
# Config
# ------------------------------
EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 600
CHUNK_OVERLAP = 150
INDEX_DIR = "./indexes"
DOC_TEXT_DIR = os.path.join(INDEX_DIR, "doc_texts")
DEFAULT_TOP_K = 5

# ------------------------------
# Utilities
# ------------------------------
def ensure_dirs():
    os.makedirs(INDEX_DIR, exist_ok=True)
    os.makedirs(DOC_TEXT_DIR, exist_ok=True)

def hash_bytes(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def pdf_to_text(pdf_bytes: bytes) -> Tuple[str, List[int]]:
    reader = PdfReader(io.BytesIO(pdf_bytes))
    texts, page_starts, cursor = [], [], 0
    for page in reader.pages:
        page_starts.append(cursor)
        t = page.extract_text() or ""
        t = "\n".join(line.strip() for line in t.splitlines())
        texts.append(t)
        cursor += len(t) + 1
    return "\n".join(texts), page_starts

def save_doc_text(doc_id: str, text: str):
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)

def load_doc_text(doc_id: str) -> str:
    path = os.path.join(DOC_TEXT_DIR, f"{doc_id}.txt")
    return open(path, "r", encoding="utf-8").read() if os.path.exists(path) else ""

def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks, i, n = [], 0, len(text)
    while i < n:
        j = min(n, i + chunk_size)
        chunks.append((text[i:j], i))
        i = j - overlap if j - overlap > i else j
    return chunks

@dataclass
class ChunkMeta:
    id: str
    doc_id: str
    start: int
    end: int
    page: int
    preview: str

class SimpleVectorIndex:
    def __init__(self, path: str, dim: int):
        self.index_path = path
        self.dim = dim
        self.faiss_index = None
        self.meta, self.id_map = {}, []

    def _create(self):
        self.faiss_index = faiss.IndexFlatIP(self.dim)

    def save(self):
        if self.faiss_index:
            faiss.write_index(self.faiss_index, self.index_path)
        with open(self.index_path + ".meta.json", "w") as f:
            json.dump({k: asdict(v) for k, v in self.meta.items()}, f)
        with open(self.index_path + ".ids", "w") as f:
            json.dump(self.id_map, f)

    def load(self) -> bool:
        if not os.path.exists(self.index_path):
            return False
        self.faiss_index = faiss.read_index(self.index_path)
        with open(self.index_path + ".meta.json") as f:
            self.meta = {k: ChunkMeta(**v) for k, v in json.load(f).items()}
        with open(self.index_path + ".ids") as f:
            self.id_map = json.load(f)
        return True

    def add(self, vectors, metas):
        if not self.faiss_index:
            self._create()
        vectors = vectors.astype("float32")
        vectors /= np.linalg.norm(vectors, axis=1, keepdims=True) + 1e-10
        self.faiss_index.add(vectors)
        for m in metas:
            self.id_map.append(m.id)
            self.meta[m.id] = m

    def search(self, query_vec, top_k=DEFAULT_TOP_K):
        q = query_vec.astype("float32")
        q /= np.linalg.norm(q) + 1e-10
        D, I = self.faiss_index.search(q.reshape(1, -1), top_k)
        return [(self.meta[self.id_map[idx]], float(score)) for score, idx in zip(D[0], I[0]) if idx != -1]

@st.cache_resource
def get_embedder():
    return SentenceTransformer(EMBED_MODEL_NAME)

def embed_texts(model, texts, batch_size=8):
    vecs = []
    for i in range(0, len(texts), batch_size):
        batch = [t for t in texts[i:i+batch_size] if t.strip()]
        if batch:
            vecs.append(model.encode(batch, convert_to_numpy=True))
    return np.vstack(vecs) if vecs else np.zeros((0, model.get_sentence_embedding_dimension()))

@st.cache_resource
def get_groq_client():
    api_key = os.getenv("GROQ_API_KEY", "")
    return Groq(api_key=api_key) if api_key and Groq else None

def call_llm(client, sys_prompt, user_prompt, model):
    if not client:
        return "[LLM disabled: no key]"
    res = client.chat.completions.create(
        model=model,
        temperature=0.2,
        max_tokens=600,
        messages=[{"role": "system", "content": sys_prompt},
                  {"role": "user", "content": user_prompt}],
    )
    return res.choices[0].message.content

def append_csv(path: str, row: Dict[str, Any]):
    df = pd.DataFrame([row])
    header = not os.path.exists(path)
    df.to_csv(path, mode="a", header=header, index=False, encoding="utf-8")

SYSTEM_PROMPT = (
    "You are an HR Policy Assistant. Use the CONTEXT to answer clearly and concisely. "
    "If context is incomplete try combining nearby chunks to give a full answer., "
    "but do not just say 'not found'."
    "If the context is irrelevant , respond clearly that you cannot answer because "
    "it is outside HR policy scope. Do not invent information."
)

# ------------------------------
# App
# ------------------------------
def main():
    st.set_page_config(page_title="HR Policy Assistant", page_icon="🧭", layout="wide")
    st.title("🧭 HR Policy Assistant Chatbot")
    st.markdown("🧑‍💼 **Your smart assistant for everything HR — fast, reliable, and always available.**")

    ensure_dirs()

    index_name = "hr_index"

    with st.sidebar:
        files = st.file_uploader("Upload HR PDFs", type="pdf", accept_multiple_files=True)
        if st.button("📂 Process Document"):
            if not files:
                st.warning("⚠️ Please upload at least one PDF before processing.")
            else:
                embedder = get_embedder()
                index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                          embedder.get_sentence_embedding_dimension())
                index._create()
                for f in files:
                    data = f.read()
                    doc_id = hash_bytes(data)[:12]
                    text, starts = pdf_to_text(data)
                    save_doc_text(doc_id, text)
                    chunks = chunk_text(text)
                    metas, texts = [], []
                    for c, s in chunks:
                        cid = str(uuid.uuid4())[:8]
                        metas.append(ChunkMeta(cid, doc_id, s, s+len(c), 0, c[:100]))
                        texts.append(c)
                    if texts:
                        vecs = embed_texts(embedder, texts)
                        index.add(vecs, metas)
                index.save()
                st.success("✅ Document processed successfully! You can now ask questions.")

    # Chat session state
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Show chat history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # --- Sample question chips ---
    st.markdown("💡 Try asking one of these:")
    cols = st.columns(3)
    sample_questions = [
    "What are the official working hours?",
    "How many earned leaves do I get?",
    "What is the notice period for resignation?"
    ]

    # Render buttons as chips
    for i, sample in enumerate(sample_questions):
      if cols[i % 3].button(sample, key=f"sample_{i}"):
        # Simulate entering the question in chat input
        st.session_state.selected_question = sample

    # --- Handle user input or chip clicks ---
    user_prompt = None

    # 1. If a chip was clicked
    if "selected_question" in st.session_state:
      user_prompt = st.session_state.pop("selected_question")

    # 2. If user typed something
    elif typed := st.chat_input("Ask me about HR policies..."):
      user_prompt = typed

    # User input
    if user_prompt:
      st.chat_message("user").markdown(user_prompt)
      st.session_state.messages.append({"role": "user", "content": user_prompt})

      embedder = get_embedder()
      index = SimpleVectorIndex(os.path.join(INDEX_DIR, index_name + ".faiss"),
                                  embedder.get_sentence_embedding_dimension())

      hr_keywords = ["leave", "leaves", "holiday", "holidays", "earned leave", "earned leaves",
                       "benefits", "working hour", "working hours", "policy", "resignation",
                       "transfer", "promotion", "attendance", "overtime", "notice period"]

      is_hr_related = any(word in user_prompt.lower() for word in hr_keywords)

      if not index.load():
            if is_hr_related:
                bot_reply = "📂 No HR policy document uploaded yet. Please upload a policy PDF."
            else:
                bot_reply = "🤖 I’m an HR Policy Assistant, and I can’t provide an answer for this query since it’s not related to HR policies."
            st.chat_message("assistant").markdown(bot_reply)
            st.session_state.messages.append({"role": "assistant", "content": bot_reply})
            return


      qvec = embed_texts(embedder, [user_prompt])[0]
      hits = index.search(qvec)

      MIN_RELEVANCE = 0.4  # lowered from 0.5 → catch more valid matches
      relevant_hits = [(m, s) for m, s in hits if s >= MIN_RELEVANCE]

      # --- Decision logic ---
      if not relevant_hits and not is_hr_related:
          bot_reply = "🤖 I’m an HR Policy Assistant, and I can’t provide an answer for this query since it’s not related to HR policies."
      else:
          # Build context from hits (if available)
          context_parts = []
          for m, _ in relevant_hits:
            doc_text = load_doc_text(m.doc_id)
            snippet = doc_text[m.start:m.end] if doc_text else m.preview
            if doc_text and not snippet.strip().endswith(('.', '!', '?')):
              extra = doc_text[m.end:m.end+200]
              snippet += " " + extra.strip().replace("\n", " ")
            context_parts.append(snippet)
          context = "\n\n".join(context_parts)

          client = get_groq_client()
          bot_reply = call_llm(client,SYSTEM_PROMPT,f"CONTEXT:\n{context}\n\nQ: {user_prompt}","llama-3.1-8b-instant")

      st.chat_message("assistant").markdown(bot_reply)
      st.session_state.messages.append({"role": "assistant", "content": bot_reply})

      # --- Feedback + Suggestion only for latest bot reply ---
      st.markdown("**Was this answer helpful?**")
      fb_col1, fb_col2 = st.columns(2)
      if fb_col1.button("👍 Yes", key=f"yes_{len(st.session_state.messages)}"):
            append_csv("feedback_log.csv", {
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
                "question": user_prompt,
                "answer": bot_reply,
                "helpful": "Yes",
            })
            st.success("Thanks for your feedback!")
      if fb_col2.button("👎 No", key=f"no_{len(st.session_state.messages)}"):
            append_csv("feedback_log.csv", {
                "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
                "question": user_prompt,
                "answer": bot_reply,
                "helpful": "No",
            })
            st.info("Feedback noted — we'll use it to improve.")

      suggestion = st.text_area(
            "💡 Any suggestions to improve this policy?",
            key=f"suggestion_{len(st.session_state.messages)}"
        )
      if st.button("Submit Suggestion", key=f"suggest_btn_{len(st.session_state.messages)}"):
            if suggestion.strip():
                append_csv("suggestion_log.csv", {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "question": user_prompt,
                    "answer": bot_reply,
                    "suggestion": suggestion.strip(),
                })
                st.success("✅ Your suggestion has been recorded and will be reviewed by the HR team.")
            else:
                st.warning("Please enter a suggestion before submitting.")

if __name__ == "__main__":
    main()